#### 02 Funcdamentals of Unconstrainted Optimization:

local minimizer $ \Rightarrow \triangledown f(x^*)=0, \triangledown ^2 f(x^*) $ is **positive semidefinite**

local minimizer $\Leftarrow \triangledown f(x^*)=0, \triangledown ^2 f(x^*)$ is **positive definite**

when $f$ is convex, any local minimizer $x^*$ is a global minimizer of $f$, if in addition $f$ is differntiable, then any stationary point $x^*$ is a global minimizer

in general, nonlinear conjugate gradient directions are much more effective than the steepest descent direction and are almost as simple to compute

poorly scaled: 对$x^*$不同维度的影响将导致*f*的巨大不同: $ f(x)=10^9 x_1^2 + x_2^2 $, 常规而言相比于line search, trust region更难修正和避免

#### 03 Line Search Methods:

line search is done in two stages: a bracketing phase finds a interval containing desirable step lengths, and a bisection or interpolation phase computes a good step length within this interval

Wolfe conditions:
$$ f(x_k+\alpha p_k) \le f(x_k) + c_1 \alpha \triangledown f_k^Tp_k \\
\triangledown f(x_k+\alpha p_k)^T p_k \ge c_2 \triangledown f_k^T p_k $$
with $ 0 < c_1 < c_2 < 1 $ 【下降了 & 下降不快了】

***
> todo: 3.2

***

#### 04 Trust-Region Methods

Cauchy point: 在梯度$g$方向上搜索二次型的最小值所得到的点
$$
\tau _k = \left\{
\begin{matrix}
1 & g_k^T B_k g_k \le 0\\
\min(||g_k||^3/(\Delta _k g_k^T B_k g_k), 1)
\end{matrix}\right.
$$

*two-dimensional subspace minimization* can be applied when $B_k$ is indefinite

###### iterative solution of the subproblem
考虑在一定条件(cost)下更为精确地求解子问题: 将带来更好的收敛性

**Theorem 4.1.** the vector $p*$ is a global solution of the trust-region problem
$$ \min _{p\in R^n} m(p) = f+g^Tp+ \frac 12 p^TBp, \quad s.t. ||p|| \le \Delta, $$
if and only if $p^*$ is feasible and there is a scalar $\lambda \ge 0$ such that the following conditions are satisfied:
$$ (B+\lambda I) p^* = -g \\
\lambda(\Delta - ||p^*||)=0 \\
(B+\lambda I) \ is\ positive\ seimidefinite.
$$
感觉就是约束问题的一阶条件+KKT约束, 对称正定则表示驻点存在且就是最优解?
通过寻找合适的$\lambda$(牛顿迭代)随后求解即可. (二次型最优解的情况: $B$正定时直接求解, $||p^*|| \le \Delta$时$\lambda=0$)
子问题的求解仍然是满足置信域半径约束的, 在理解里面, 这是与LM不同之处

$f$ poorly scaled时, 球形的置信域是不太合适的, 修改为$||Dp|| \le \Delta$, 对角阵$D$的选择可以参考二阶导$\partial ^2f/ \partial x_i^2$, LM中正是使用了$diag(J^TJ)$以达到类似的效果

#### 05 Conjugate Gradient Methods:

a set of nonzero vectors $\{p_0, p_1, ..., p_l\}$ is said to be **conjugate** with respect to the symmetric positive definite matrix $A$ if
$$p_i^T A p_j=0, \quad for \ all \ i \not = j$$

考虑问题$Ax=b$， $A$正定, 即$\min \phi (x) = \frac 12 x^TAx-b^Tx$任意起点, 可在$n$步内获得结果

共轭方向法(conjugate direction algorithm)最多$n$次迭代终止, 直观上讲, 共轭方向法相当于添加了转换关系将目标函数“对齐”到直角坐标轴方向: 通过$\hat x = S^{-1}x, \ S=[p_0\ p_1\ ...\ p_{n-1}]$, 获得对角的$S^T A S$, 从而“逐方向求解”

共轭梯度法(conjugate gradient method)
初始方向为负梯度, 随后每次选择与上次共轭的方向$p_k$, $p_k$的选择为上次方向与当前负梯度的方向线性组合, 即:
$$p_k = -r_k+\beta _k p_{k-1}$$
左乘$p_{k-1}^TA$即可计算$\beta_k$

the CG method is recommended only for large problems; otherwise, Gaussian elimination or other factorization algorithms such as the singular value decomposition are to be preferred, since they are less sensitive to rounding errors;

$A$有$r$个不同的特征值, 则最多经过$r$次迭代, 算法的收敛速度和$A$特征值分布相关, $A$的条件数越大收敛越慢

the Fletcher-Reeves method: 将共轭梯度法扩展到一般凸优化问题时, [1]步长由line search的方式进行, [2]使用$f$的梯度$\nabla f$代替$r_k = Ax_k-b$:
1. $p_0=-\nabla f(x_0), \ k=0$
2. compute $\alpha_k$: $x_{k+1}=x_k+\alpha_k p_k$
3. compute $\beta_{k+1}$: $\beta_{k+1}= \frac {\nabla f_{k+1}^T \nabla f_{k+1}}
{\nabla f_k^T \nabla f_k}$,
4. $p_{k+1} = -\nabla f_k + \beta_{k+1} p_k$
5. goto 2

#### 06 Quasi-Newton Methods:

#### 07 Large-Scale Unconstrained Optimization:

***
> todo: 7.2

***

#### 10 Least-Squares Problems

#### 15 Fundamentals of Algorithms for Nonliear Constrained Optimization:

a nonlinear optimization problem with linear equality constraints is, from a methematical point of view, the same as an unconstrained problem

Maratos effect: some algorithms based on merit functions or filters may fail to converge rapidly beacause they reject steps that make good progress toward a solution

#### 16 Quadratic Programming

等式约束:
$$ \min_x \ q(x)= \frac 12 x^TGx+x^Tc \\
s.t. \ Ax=b
\tag {16-1}
$$

KKT条件:
$$ \begin{bmatrix} G & -A^T \\ A & 0 \end{bmatrix}
\begin{bmatrix} x^* \\ \lambda^* \end{bmatrix} = \begin{bmatrix} -c \\ b \end{bmatrix}
\tag {16-2}
$$

记，$Z$为$A$零空间的一组基: $AZ=0$
* let $A$ have full row rank, $Z^TGZ$ is positive definite, then:
$$ K= \begin{bmatrix} G & A^T \\ A & 0 \end{bmatrix}$$
is nonsingular, and hence there is a unique vector pair $(x^*, \lambda^*)$ satisfying (16-2)

#### 17 Penalty and Augmented Lagrangian Methods

**Quadratic Penalty Method:**
$$ Q(x; \mu) = f(x)+ \frac \mu2 \sum_{i \in E} c_i^2(x) +
\frac \mu2 \sum_{i \in I} ([c_i(x)]^-)^2 $$
其中: $[y]^- = \max (-y, 0) $

求解时, 根据前面迭代的结果选择本次迭代初值获得(近似)最小值, 不断提高罚因子$\mu$, 收敛时即获得解

























---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---


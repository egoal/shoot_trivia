#### 02:

local minimizer $ \Rightarrow \triangledown f(x^*)=0, \triangledown ^2 f(x^*) $ is **positive semidefinite**

local minimizer $\Leftarrow \triangledown f(x^*)=0, \triangledown ^2 f(x^*)$ is **positive definite**

when $f$ is convex, any local minimizer $x^*$ is a global minimizer of $f$, if in addition $f$ is differntiable, then any stationary point $x^*$ is a global minimizer

in general, nonlinear conjugate gradient directions are much more effective than the steepest descent direction and are almost as simple to compute

poorly scaled: changes to *x* in a certain direction produce much larger variations in the value of *f* than do changes to *x* in another direction:
$$ f(x) = 10^9x_1^2+x_2^2 $$

generally speaking, it's easier to preserve scale invariance for line search algorithms than for trust-region algorithms

#### 15:

a nonlinear optimization problem with linear equality constraints is, from a methematical point of view, the same as an unconstrained problem

Maratos effect: some algorithms based on merit functions or filters may fail to converge rapidly beacause they reject steps that make good progress toward a solution

#### 16. Quadratic Programming

等式约束:
$$ \min_x \ q(x)= \frac 12 x^TGx+x^Tc \\
s.t. \ Ax=b
\tag {16-1}
$$

KKT条件:
$$ \begin{bmatrix} G & -A^T \\ A & 0 \end{bmatrix}
\begin{bmatrix} x^* \\ \lambda^* \end{bmatrix} = \begin{bmatrix} -c \\ b \end{bmatrix}
\tag {16-2}
$$

记，$Z$为$A$零空间的一组基: $AZ=0$
* let $A$ have full row rank, $Z^TGZ$ is positive definite, then:
$$ K= \begin{bmatrix} G & A^T \\ A & 0 \end{bmatrix}$$
is nonsingular, and hence there is a unique vector pair $(x^*, \lambda^*)$ satisfying (16-2)



























---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---




均值与方差的描述:
$$ \bar X = \sum X_i/n,\quad S^2 = \frac 1{n-1}\sum (\bar X-X_i)^2 $$
n个自由度，由于均值已经保有一个自由度, 因此为$n-1$. $\bar X$带入$\sum (X_i-\bar X)$, 整理为二次型, $\sum_{i, j=1}^n a_{ij}X_iX_j$, 则方阵$A=(a_{ij})$的秩为$n-1$

##### 泰勒展开
$$ f(x)= \sum _{i=0}^n \frac {f^{(i)}(a)}{i!} (x-a)^i + R_n(x) $$
皮亚诺余项描述了接近程度:

$$ R_n(x) = o[(x-a)^n] $$

拉格朗日余项为微分中值定理推广:

$$ R_n(x) = \frac {f^{(n+1)}(\theta)}{(n+1)!} (x-a)^{n+1} $$

积分余项为微积分基本定理的推广:

$$ R_n(x) = \int _a^x \frac {f^{(n+1)}(t)}{n!}(x-t)^n\ dt $$

余项估计: 若$|f^{(n+1)}(x)| \le M_n, \forall \ x \in (a-r, a+r)$, 则

$$|R_n(x)| \le M_n \frac {r^{n+1}}{(n+1)!} $$

##### 利普希茨连续(Lipschitz continuity)
对于在实数集的子集的函数$f: D \subseteq \mathbb R \rightarrow \mathbb R$, 若存在常数$K$使得: $|f(a)-f(b)| \leq K |a-b|, \forall a, b \in D$. 称$f$符合利普希茨条件

比连续更强的光滑性条件, 限制了函数改变的速度, 符合利普希茨条件的函数的斜率, 必小于一个称为利普希茨常数的实数.
$f: [-3, 7] \rightarrow \mathbb R, \ f(x)=x^2$符合条件: $K=14$

---

**矩阵基本**：

$AC^T =|A| \ \mathbf I$, $C$为伴随矩阵

正交矩阵: $Q^TQ=I$, 线性映射, 正交矩阵保持距离不变, 是一个保距映射(旋转、镜射)

条件数:
$$ cond(A) = ||A|| \ ||A^{-1}|| $$
方阵$A$奇异即条件数为$\infty$. 条件数描述了其对应线性系统的稳定性或敏感程度,

##### 范数
假设$V$是域$\mathbf F$上的向量空间, $V$的**半范数**是一个函数$p: V \rightarrow \mathbb R; x \rightarrow p(x)$, 满足:
$\forall a \in \mathbf F, \forall u, v \in V$,
1. 正定: $p(v) \ge 0$
2. 齐次: $p(av)=|a|\ p(v)$
3. 三角不等式: $p(u+v) \le p(u)+p(v)$

**范数**则加上一个额外性质: $p(v)=0 \Leftrightarrow v=\mathbf 0$
矩阵范数还规定满足相容性: $||AB|| \le ||A||\ ||B||$, 引入相容性是为了保持矩阵作为线性算子的特征

* F范数

$$ \lVert A \rVert _F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n \lvert a_{ij} \rvert ^2} = \sqrt {tr(A^TA)} $$

* p范数
$$ ||A||_p = \max_{||x||_p \ne 0} \frac {||Ax||_p}{||x||_p}=
\max_{||x||_p=1}||Ax||_p $$
$$||A||_1 = \max_{j\in n} \sum_{i=1}^m|a_{ij}|$$
$$||A||_{\infty} = \max_{i\in m} \sum_{j=1}^n|a_{ij}|$$
$$||A||_2 = \lambda_{max}^{\frac 12}(A^TA)= \lambda_{max}^{\frac 12}(AA^T)
=\sigma_1(A) $$

**正定**:

实对称矩阵$A_{n\times n}$:
正定 $\iff$ 合同于单位矩阵 or 特征值大于0 or $X^TAX$的正惯性指数$=n$ or 顺序主子式都大于0
半正定 $\iff$ 合同于分块矩阵$(E_r, 0; 0, 0)$特征值大于0且至少有一个=0 or $X^TAX$的正惯性指数$p< n$

**矩阵分解**:
- **对角化:** 
$S^{-1}AS = \Lambda$, $S$由特征向量组成, 需要所有特征向量相互独立, 即特征值互不相同

* **LU分解:**
$A=LU$, 对应于高斯消元由上至下消去获得*行梯阵式*, 求解线性方程组时即由下至上进行(消元获得)

* **QR分解:**
将**列满秩**矩阵分解为$A=QR$, 获得正交阵$Q: Q^TQ=I$(将$A$的列不断投影获得一组正交基即得到$Q$)

* **Cholesky分解:**
奖对称正定阵分解为$A=LDL^T$, 对实对称矩阵存在特征分解$A=Q\Lambda Q^T$

对于对阵矩阵来说:
1) 有实特征值和正交的特征向量
2) 对角化: $A = Q\Lambda Q^T, Q$为正交阵
3) 即使存在重复的特征值也可以对角化
4) 特征值符号与pivots相同

**特征值**:

$$ \left \{ \begin{matrix} \sum_i \lambda_i = tr(A) \\ \prod _i \lambda_i = |A|
\end{matrix} \right.$$

$$ since \quad |A-\lambda I| = \prod_i (\lambda_i-\lambda)=0 $$

对称阵不同特征值对应的特征向量垂直:

$$ \lambda x^T y = x^T A^Ty = x^T \mu y \Rightarrow (\lambda - \mu)x^T y = 0$$

矩阵$A$正定当且仅当其所有特征值为正

对任意特征向量$r$: $0 < r^TAr = r^T\lambda r = \lambda \ ||r||^2 \Rightarrow \lambda > 0$

对任意$x$: 记$x = \sum \alpha_i r_i$, 则$x^TAx = x^T \sum \lambda_i \alpha_i r_i \ge (\min \lambda_i)\ ||x||^2>0$


**伪逆**:

对$A: \mathcal X \to \mathcal Y$, 定义$T: \mathcal N(A)^\perp  \to \mathcal R(A)$为:
$$ Tx = Ax, \forall x \in \mathcal N(A)^\perp $$
由此定义伪逆$A^+: \mathcal Y \to \mathcal X$
$$ A^+y = T^{-1}y_1$$
其中$y_1$为$y \in R^n$在$\mathcal{R}(A)$中的分量(即忽视正交补中的分量)

常规svd分解求伪逆:
$$A^+ = V\Sigma^+ U^T, \quad
\Sigma^+ = [\sigma^{-1}, ..., 0, ...] $$
qr分解求伪逆:
$$A^+ = R^{-1}Q^T$$

**矢量化**:
$$vec(XY) = (Y^T \otimes I_k) \ vec(X) $$
$$vec(AYC) = (C^T \otimes A) \ vec(Y) $$

酉(unitary): $A^HA=I$

**矩阵最小二乘**:
$$AX = B; \quad A \in R^{m \times n}, B \in R^{m\times k}$$
通解:
$$X = A^+B + (I-A^+ A)Y, \quad \forall \ Y\in R^{n\times k}$$
其中$A^+B$为使得$Tr X^TX$取最小值

2范数$k$秩近似:
$$ \min_{M\in R_k^{m\times n}} ||A-M||_2 \quad \rightarrow \quad M= \sum_{i=1}^k \sigma_i u_i v_i^T $$
特别地，对$n$阶方阵计算的$k-1$阶近似为其最近的奇异阵

**几个常用公式**:
$$ (A+BDC)^{-1} = A^{-1}-A^{-1}B(D^{-1}+CA^{-1}B)^{-1}CA^{-1} $$
$$ \begin{bmatrix} A& B \\ 0& D \end{bmatrix}^{-1} = 
\begin{bmatrix} A^{-1}& -A^{-1}BD^{-1} \\ 0& D^{-1} \end{bmatrix} $$
$$ \begin{bmatrix} A& 0 \\ C& D \end{bmatrix}^{-1} = 
\begin{bmatrix} A^{-1}& 0 \\ -D^{-1}CA^{-1}& D^{-1} \end{bmatrix} $$

> a linear transformation $P$ is a projection *if and only if* it is **idempotent(幂等)**, i.e. $P^2=P$. also, $P$ is a projection *if and only if* $I-P$ is a projection. in fact: $P_{\mathcal Y, \mathcal X}=I-P_{\mathcal X, \mathcal Y}$. 

$$ <x, Ay>_Q = <Bx, y>_R \quad \rightarrow \quad B = R^{-1}A^TQ$$

> let $A \in R_r^{m\times n}$ with SVD: $A=U\Sigma V^T= \sum_{i=1}^r 
\sigma_i u_i v_i^T$, then a best rank k approximaition to $A$ for $1\le k\le r$, i.e., a solution to $\min_{M\in R_k^{m\times n}} ||A-M||_2$, is given by 
$M_k = \sum_{i=1}^k \sigma_i u_i v_i^T$.

> if $\lambda$ is a root ot *mutiplicity* $m$ of $\pi(\lambda)$, we say that $\lambda$ is an eigenvalue of $A$ of **algebraic multiplicity** $m$. the **geometric multiplicity** of $\lambda$ is the number of associated independent eigenvectors $=n- rank(A-\lambda I) = \dim \mathcal{N}(A-\lambda I)$

> if $f$ is a analytic function, then the eigenvalues of $f(A)$ are $f(\lambda)$, buf $f(A)$ does not necessarily have all the same eigenvectors, unless, say, $A$ is **diagonalizable**.

**kronecker**:
$$(A\otimes B)(C\otimes D) = AC\otimes BD$$
$$(A\otimes B)^T = A^T\otimes B^T$$
$$(A\otimes B)^{-1} = A^{-1}\otimes B^{-1}$$
$$A\oplus B = I_m\otimes A+ B\otimes I_n$$

> a complex square matrix $A$ is **normal** if: $A^*A = AA^*$

---

**$\chi^2$ distribution**: 
$$ f(x)= \frac 1{\Gamma(p/2)\ 2^{p/2}} x^{p/2-1} e^{-x/2} $$
if $Z_1, ..., Z_p$ are independent standard Normal random variables, then $\sum_{i=1}^p Z_i^2 \sim \chi_p^2$

> suppose that the range of $X$ and $Y$ is a (possibly infinite) *rectangle*, if $f(x, y)=g(x)h(y)$ for some functions $g$ and $h$ (not neccesarily probability density functions) then $X$ and $Y$ are independent

**multivariate normal(多元正态分布)**:

let $X\sim N(\mu, \Sigma)$, then:
1. *marginal distribution*: $X_a\sim N(\mu_a, \Sigma_{aa})$
2. *conditional distribution*: $(X_b|X_a = x_a)\sim N(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a),\quad \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab})$
3. if a is a vector then $a^TX\sim N(a^T\mu, a^T\Sigma_a)$
4. $V=(X-\mu)^T\Sigma^{-1}(X-\mu)\sim \chi _k^2$

**expectations and variance**:
$$ E(\sum_i a_i X_i) = \sum_i a_i E(X_i) $$
$$ E(\prod_i X_i) = \prod_i E(X_i) \quad \quad independ\ required$$
$$ V(X) = E(X^2)-\mu ^2 $$
$$ V(aX+b) = a^2 V(x) $$
$$ V(\sum_i a_i X_i) = \sum_i a_i^2 V(X_i)+ \sum_i \sum_j Cov(X_i, X_j) $$

> $X$ is a random vector with mean $\mu$ and variance $\Sigma$ then, if $A$ is a matrix(vector) then $E(AX) = AE(X)$ and $V(AX) = A\Sigma A^T$

**moment generating function**, or **laplace transform**, of $X$ is defined by: 
$$\psi_X(t)= E(e^{tX})= \int e^{tx}dF(x)$$
where $t$ varies over the real numbers. give:
$$\psi_X'(t)|_{t=0} = E(X), \quad \psi_X''(t)|_{t=0} = E(X^2)$$
if $\psi_X(t)=\psi_Y(t)$ for all $t$ in an open interval around $0$, then $X\overset d=Y$

**inequality**:
> Markov, for *non-negative* X: 
$$P(X>t) \le \frac{E(X)}t$$
> Chebyshev
$$ P(|x-\mu|\ge t) \le \frac {\sigma^2}{t^2}, \quad P(|Z|\ge k)\le \frac 1{k^2} $$
> Hoeffding, \
> for *independent observations* $\{Y_i\}$ with $E(Y_i)=0$ and $a_i\le Y_i\le b_i$, let $\epsilon>0$:
$$P(\sum_{i=1}^n Y_i\ge \epsilon)\le e^{-t\epsilon} \prod_{i=1}^n e^{t^2(b_i-a_i)^2/8}, \quad \forall t>0$$ 
> let $X_i\sim Bernolli(p)$, then $\overline{X_n}= n^{-1}\sum X_i$:
$$ P(|\overline X_n - p|>\epsilon) \le 2e^{-2n\epsilon^2}, \quad \forall \epsilon>0$$
> Cauchy-Schwarz, for $X$, $Y$ with *finite variances*:
$$E|XY| \le \sqrt{E(X^2)E(Y^2)}$$
> Jensen, for *convex* $g$:
$$E(g(X)) \ge g(E(X))$$

**the central limit theorem(CLT)**:
let $X_i$ be **IID** with mean $\mu$ and variance $\sigma^2$, let $\overline{X_n}=n^{-1}\sum_{i=1}^n X_i$. then: $\overline{X_n} \approx N(\mu, \sigma^2/n)$
